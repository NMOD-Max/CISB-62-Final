{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n# Import necessary libraries\n# Import necessary libraries\n# ...\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Load a smaller portion of the dataset (e.g., first 1000 rows)\ndf = pd.read_csv('../input/amazon-fine-food-reviews/Reviews.csv', nrows=1000)\n\n# Handling missing values\ndf.dropna(inplace=True)\n\n# Descriptive statistics\nprint(\"Descriptive Statistics:\")\nprint(df.describe())\n\n# Display the first few rows of the dataset\nprint(\"\\nHead of the Dataset:\")\nprint(df.head())\n\n# Display information about the dataset\nprint(\"\\nDataset Information:\")\nprint(df.info())\n\n\n# Data Transformation\ndf['Text_Length'] = df['Text'].apply(len)\n\n# Visualizations using Matplotlib and Seaborn\n# Visualize the distribution of review scores\nplt.figure(figsize=(8, 6))\nsns.countplot(x='Score', data=df)\nplt.title('Distribution of Review Scores')\nplt.xlabel('Score')\nplt.ylabel('Count')\nplt.show()\n\n# Visualize the distribution of text lengths\nplt.figure(figsize=(8, 6))\nsns.histplot(df['Text_Length'], bins=30, kde=True)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()\n\n# Text classification using Naive Bayes\ntext_data = df['Text']\nlabels = df['Score'].apply(lambda score: 1 if score > 3 else 0)\n\n#\n# Tokenize and pad the sequences\nmax_words = 500\nmax_len_values = [30, 50, 70]  # Adjust these values based on your experimentation\n\n# Initialize the Tokenizer\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(text_data)\nsequences = tokenizer.texts_to_sequences(text_data)\npadded_sequences = pad_sequences(sequences, maxlen=max_len_values[-1], padding='post', truncating='post')\n\n# Train-test split for Naive Bayes\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(text_data, labels, test_size=0.2, random_state=101)\n\n# Naive Bayes using CountVectorizer\nvectorizer = CountVectorizer()\ntrain_features = vectorizer.fit_transform(train_texts)\ntest_features = vectorizer.transform(test_texts)\n\nclassifier = MultinomialNB()\nclassifier.fit(train_features, train_labels)\npredictions_nb = classifier.predict(test_features)\n\n# Evaluate Naive Bayes\naccuracy_nb = accuracy_score(test_labels, predictions_nb)\nprint(\"Naive Bayes Accuracy:\", accuracy_nb)\nprint(\"Classification Report (Naive Bayes):\\n\", classification_report(test_labels, predictions_nb))\n\n# Define early stopping and model checkpoint callbacks for the LSTM model\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n\n# Define the LSTM model with callbacks\ndef create_lstm_model(embedding_dim=50, lstm_units=32):\n    model = Sequential([\n        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len_values[-1]),\n        LSTM(lstm_units),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# Train the LSTM model with callbacks\n# Train the LSTM model with callbacks\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\nlstm_model = create_lstm_model()\nlstm_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[early_stopping, model_checkpoint])\n\n\n# Evaluate the LSTM model\nlstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\nprint(f\"LSTM Model Accuracy: {lstm_accuracy}\")\n\n# Load the best model from the checkpoint\nbest_lstm_model = create_lstm_model()\nbest_lstm_model.load_weights('best_lstm_model.h5')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-10T07:06:08.737844Z","iopub.execute_input":"2023-12-10T07:06:08.738229Z","iopub.status.idle":"2023-12-10T07:06:16.593079Z","shell.execute_reply.started":"2023-12-10T07:06:08.738202Z","shell.execute_reply":"2023-12-10T07:06:16.592136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use best_lstm_model for predictions\nsample_texts = [\"This product is amazing!\", \"The Food was great.\"]\nsample_sequences = tokenizer.texts_to_sequences(sample_texts)\nsample_padded_sequences = pad_sequences(sample_sequences, maxlen=max_len_values[-1], padding='post', truncating='post')\n\nsample_predictions = best_lstm_model.predict(sample_padded_sequences)\nfor i, text in enumerate(sample_texts):\n    sentiment = \"Positive\" if sample_predictions[i] > 0.5 else \"Negative\"\n    print(f\"Review: {text}\\nPredicted Sentiment: {sentiment}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T07:06:30.831390Z","iopub.execute_input":"2023-12-10T07:06:30.831739Z","iopub.status.idle":"2023-12-10T07:06:31.216434Z","shell.execute_reply.started":"2023-12-10T07:06:30.831713Z","shell.execute_reply":"2023-12-10T07:06:31.215019Z"},"trusted":true},"execution_count":null,"outputs":[]}]}